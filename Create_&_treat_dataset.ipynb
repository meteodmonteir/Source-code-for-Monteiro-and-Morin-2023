{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/monteirod/script/utils/')\n",
    "import utils as u\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "import regionmask\n",
    "import shapefile as shp\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from glob import glob\n",
    "import os\n",
    "import datetime\n",
    "from scipy import stats\n",
    "import sys\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Resampled spatially, temporally and save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Homogenized in terms of spatial extent and averaged monthly the different datasets\n",
    "### then save it in a repository\n",
    "### The targeted format is dimension : time, spatial_dim / coordinates : lon, lat, ZS (orography) / variables : variable \n",
    "\n",
    "data_path = '/cnrm/cen/users/NO_SAVE/monteirod/' # Roots for the different repertory used\n",
    "shp_path = '/cnrm/cen/users/NO_SAVE/monteirod/Shapefile/NUTS0_UE_ALPS' # Location of the used shapefile\n",
    "shp = geopandas.read_file(shp_path) # Read shapefile with geopandas\n",
    "\n",
    "## Boundaries of the domain to be subset : Alpine ridge extension\n",
    "lat_inf, lat_sup = 43,49\n",
    "lon_inf, lon_sup = 4,17\n",
    "\n",
    "## Path of the outputs\n",
    "out_reportory = '/cnrm/cen/users/NO_SAVE/monteirod/dataset_monthly/'\n",
    "\n",
    "## Enable/disable SCD (snow cover duration) / SD (snow duration)\n",
    "sd = False\n",
    "scd = True\n",
    "threshold_scd = 0.05 # Threshold used for snow depth (m) to determine whether there is snow or not\n",
    "\n",
    "## List of the dataset on which the program will applied\n",
    "ls_dataset = ['OBS_insitu','MTMSI','UERRA','AROME','ERA5','ERA5_CROCUS','ERA5_Land', 'CERRA_Land'] # full list\n",
    "# ls_dataset = ['CERRA_Land']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBS_insitu\n",
      "OBS_insituscd\n",
      "[########################################] | 100% Completed | 951.93 ms\n",
      "MTMSI\n",
      "MTMSIscd\n",
      "[########################################] | 100% Completed | 1.17 sms\n",
      "UERRA\n",
      "UERRAscd\n",
      "[########################################] | 100% Completed | 157.85 s\n",
      "AROME\n",
      "AROMEscd\n",
      "[########################################] | 100% Completed | 192.16 s\n",
      "ERA5\n",
      "ERA5scd\n",
      "[########################################] | 100% Completed | 6.80 ss\n",
      "ERA5_CROCUS\n",
      "ERA5_CROCUSscd\n",
      "[########################################] | 100% Completed | 243.19 s\n",
      "ERA5_Land\n",
      "ERA5_Landscd\n",
      "[########################################] | 100% Completed | 19.10 s\n",
      "CERRA_Land\n",
      "CERRA_Landscd\n",
      "[########################################] | 100% Completed | 65.15 s\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(ls_dataset)):\n",
    "    dataset = ls_dataset[i]\n",
    "    var = 'snd' # Specify which variable, snd (snow depth), tas (temperature), pr (precipitation)\n",
    "    print(dataset)\n",
    "\n",
    "    ### Obs matiu ###\n",
    "    \n",
    "    if dataset == 'OBS_insitu' and var == 'snd':\n",
    "        ds = xr.open_mfdataset(data_path+'Reference_Dataset/SND_OBS/OBS_Matiu/snd_Alps_1961_2020.nc', chunks = 'auto') # Open file\n",
    "        \n",
    "        # Pre-treatment\n",
    "        ds = ds.drop(['HS_after_qc'])\n",
    "        ds = u.sellonlatbox(ds,['Station_Name'],lat_inf,lat_sup,lon_inf,lon_sup, avoid_reindex=False) # Resampled spatially\n",
    "        ds['snd'] = ds['snd']/100 # cm --> m\n",
    "        \n",
    "        if scd == True:\n",
    "            var = 'scd'\n",
    "            ds = ds['snd'].sel(time = slice('1985','2015')).dropna(dim = 'Station_Name').resample(time = 'A-SEP', closed = 'left',label = 'left').map(u.lcscd, threshold = threshold_scd)\n",
    "        \n",
    "        elif sd == True:\n",
    "            var = 'sd'\n",
    "            ds = ds['snd'].sel(time = slice('1985','2015')).dropna(dim = 'Station_Name').resample(time = '1MS', closed = 'left',label = 'left').map(u.lcscd, threshold = threshold_scd)\n",
    "        \n",
    "        else:\n",
    "            ds = ds.resample(time = '1MS', closed = 'left',label = 'left').mean()\n",
    "\n",
    "    ### LAPrec ###\n",
    "\n",
    "    if dataset == 'LAPrec' and var == 'pr':\n",
    "        ds = xr.open_mfdataset(data_path+'Reference_Dataset/LAprec/LAPrec1901.v1.1.nc', chunks = 'auto')\n",
    "\n",
    "        # Pre-treatment\n",
    "        ds = ds.drop('lambert_azimuthal_equal_area').set_coords(['lat','lon','dem']).rename({\"LAPrec1901\":\"pr\", 'dem':'ZS','X':'x','Y':'y'})  \n",
    "        ds['pr'] = ds['pr']/ds.time.dt.days_in_month ## transform monthly pr sum to daily weight by number of day in month\n",
    "\n",
    "        ds = u.sellonlatbox(ds,['x','y'],lat_inf,lat_sup,lon_inf,lon_sup)\n",
    "        ds = ds.resample(time = '1MS', closed = 'left',label = 'left').mean()\n",
    "\n",
    "    ### E-OBS tas ###\n",
    "\n",
    "    if dataset == 'E_OBS' and var == 'tas':\n",
    "        ds = xr.open_mfdataset(data_path+'Reference_Dataset/E-OBS_0.1degree/tas/monthly/tg_ens_mean_0.1deg_reg_v23.1e_monthly.nc', chunks = 'auto')\n",
    "\n",
    "        # Pre-treatment\n",
    "        ds_spread = xr.open_mfdataset(data_path+'Reference_Dataset/E-OBS_0.1degree/tas/monthly/tg_ens_spread_0.1deg_reg_v23.1e_monthly.nc', chunks = 'auto').rename({'tg':'tas_spread'})\n",
    "        ds_ZS = xr.open_mfdataset(data_path+'Reference_Dataset/E-OBS_0.1degree/fx/*.nc')\n",
    "        \n",
    "        # Rounded issues between the orography files and the other, so we have to round it ourself\n",
    "        ds['latitude'] = ds['latitude'].round(decimals = 5)\n",
    "        ds['longitude'] = ds['longitude'].round(decimals = 5)\n",
    "        ds_ZS['latitude'] = ds_ZS['latitude'].round(decimals = 5)\n",
    "        ds_ZS['longitude'] = ds_ZS['longitude'].round(decimals = 5)\n",
    "        ds_spread['latitude'] = ds_spread['latitude'].round(decimals = 5)\n",
    "        ds_spread['longitude'] = ds_spread['longitude'].round(decimals = 5)\n",
    "        \n",
    "        ds = xr.merge([ds,ds_ZS, ds_spread])\n",
    "        ds = ds.rename({\"tg\":\"tas\",'latitude':'lat','longitude':'lon', 'elevation':'ZS'}).drop('time_bnds').set_coords('ZS')\n",
    "\n",
    "        ds = u.sellonlatbox(ds,['lat','lon'],lat_inf,lat_sup,lon_inf,lon_sup, avoid_reindex=False)\n",
    "        ds = ds.resample(time = '1MS', closed = 'left',label = 'left').mean()\n",
    "    \n",
    "    ### E-OBS tas HOM ###\n",
    "    \n",
    "    if dataset == 'E_OBS_HOM' and var == 'tas':\n",
    "        ds = xr.open_mfdataset(data_path+'Reference_Dataset/E-OBS_0.1degree/tas_homogenized/monthly/tg_ens_mean_monthly_0.1deg_reg_v19.0eHOM.nc', chunks = 'auto')\n",
    "\n",
    "        # Pre-treatment\n",
    "        ds_ZS = xr.open_mfdataset(data_path+'Reference_Dataset/E-OBS_0.1degree/fx/*.nc')\n",
    "        \n",
    "        # Rounded issues, so we have to round it ourself\n",
    "        ds['latitude'] = ds['latitude'].round(decimals = 5)\n",
    "        ds['longitude'] = ds['longitude'].round(decimals = 5)\n",
    "        ds_ZS['latitude'] = ds_ZS['latitude'].round(decimals = 5)\n",
    "        ds_ZS['longitude'] = ds_ZS['longitude'].round(decimals = 5)\n",
    "        \n",
    "        ds = xr.merge([ds,ds_ZS])\n",
    "        ds = ds.rename({\"tg\":\"tas\",'latitude':'lat','longitude':'lon', 'elevation':'ZS'}).drop('time_bnds').set_coords('ZS')\n",
    "\n",
    "        ds = u.sellonlatbox(ds,['lat','lon'],lat_inf,lat_sup,lon_inf,lon_sup, avoid_reindex=False)\n",
    "        ds = ds.resample(time = '1MS', closed = 'left',label = 'left').mean()\n",
    "\n",
    "    ### MTMSI ### \n",
    "\n",
    "    if dataset == 'MTMSI':\n",
    "        \n",
    "        if var == 'snd':\n",
    "            ds = xr.open_mfdataset(data_path+'UERRA_CLIMTOUR_NN/tas_pr_snd/PRO_*.nc', chunks = 'auto').rename({'DSN_T_ISBA':'snd'}).drop(['latitude','longitude','ZS']) # Drop lat, lon, ZS because they contain false values\n",
    "\n",
    "        elif var == 'tas' or var == \"pr\":\n",
    "            ds = xr.open_mfdataset(data_path+'UERRA_CLIMTOUR_NN/tas_pr_snd/FORCING_UERRA_*.nc', chunks = 'auto').drop(['LAT','LON','ZS'])\n",
    "\n",
    "        ds_ZS = xr.open_dataset(data_path+'UERRA_CLIMTOUR_NN/tas_pr_snd/FORCING_UERRA_198208010600_1983080106000_EUROPE.nc').isel(time = 0).squeeze()[['ZS', 'LAT','LON']] # Real lat, lon, ZS values\n",
    "        ds = xr.merge([ds,ds_ZS])\n",
    "\n",
    "        ## Pre-treatment\n",
    "        ds = ds.rename({'LAT':'lat','LON':'lon'}).set_coords(['ZS','lat','lon'])\n",
    "        ds = u.sellonlatbox(ds,['Number_of_points'],lat_inf,lat_sup,lon_inf,lon_sup)\n",
    "\n",
    "        if scd == True:\n",
    "            var = 'scd'\n",
    "            ds = ds['snd'].resample(time = 'A-SEP', closed = 'left',label = 'left').map(u.lcscd, threshold = threshold_scd)\n",
    "            \n",
    "        elif sd == True:\n",
    "            var = 'sd'\n",
    "            ds = ds['snd'].resample(time = '1MS', closed = 'left',label = 'left').map(u.lcscd, threshold = threshold_scd)\n",
    "        \n",
    "        else:\n",
    "            ds = ds.resample(time = '1MS', closed = 'left',label = 'left').mean()\n",
    "\n",
    "        ## Post treatment \n",
    "        if var == 'tas':\n",
    "            ds['tas'] = ds['Tair'] - 273.15\n",
    "            ds = ds[['tas','ZS']]\n",
    "            \n",
    "        elif var == 'pr':\n",
    "            ds['pr'] = ds['Rainf']*3600*24 + ds['Snowf']*3600*24\n",
    "            ds = ds[['pr','ZS']]    \n",
    "\n",
    "    ### UERRA ### \n",
    "\n",
    "    if dataset == 'UERRA':\n",
    "        \n",
    "        if var == 'tas' or var == 'pr':\n",
    "            ds = xr.open_mfdataset(data_path+'UERRA_MESCAN_SURFEX/MESCAN-SURFEX_an/'+var+'/'+var+'_UERRA_MESCAN_SURFEX_day_*.nc', chunks = 'auto', combine = 'nested',concat_dim='time', coords = 'minimal', data_vars='minimal')\n",
    "        \n",
    "        elif var == 'snd':\n",
    "            ds = xr.open_mfdataset(data_path+'UERRA_MESCAN_SURFEX/SURFEX_fc/'+var+'/'+var+'_UERRA_MESCAN_SURFEX_day_*.nc', chunks = 'auto', combine = 'nested',concat_dim='time', coords = 'minimal', data_vars='minimal')\n",
    "\n",
    "        ## Pre-treatment\n",
    "        if var == 'tas':\n",
    "            ds = ds.drop(['height','time_bnds','LAT','LON']).squeeze() # Forcings are in SURFEX format, so we transform it\n",
    "\n",
    "        ds_ZS = xr.open_dataset('/cnrm/cen/users/NO_SAVE/monteirod/UERRA_MESCAN_SURFEX/fx/ZS_lon_lat_UERRA_MESCAN.nc')\n",
    "        ds = xr.merge([ds_ZS,ds]).rename({'LAT':'lat','LON':'lon'}).set_coords(['ZS','lat','lon'])\n",
    "        ds = u.sellonlatbox(ds,['x','y'],lat_inf,lat_sup,lon_inf,lon_sup)\n",
    "\n",
    "        if scd == True:\n",
    "            var = 'scd'\n",
    "            ds = ds['snd'].resample(time = 'A-SEP', closed = 'left',label = 'left').map(u.lcscd, threshold = threshold_scd)\n",
    "            \n",
    "        elif sd == True:\n",
    "            var = 'sd'\n",
    "            ds = ds['snd'].resample(time = '1MS', closed = 'left',label = 'left').map(u.lcscd, threshold = threshold_scd)\n",
    "        \n",
    "        else:\n",
    "            ds = ds.resample(time = '1MS', closed = 'left',label = 'left').mean()\n",
    "\n",
    "        ## Post-treatment \n",
    "        if var == 'tas':\n",
    "            ds =  ds.rename({'2t':'tas'})\n",
    "            ds['tas'] = ds['tas'] - 273.15\n",
    "            ds = ds[['tas','ZS']]\n",
    "        \n",
    "        elif var == 'pr':\n",
    "            ds =  ds.rename({'tp':'pr'})\n",
    "            ds = ds[['pr','ZS']]\n",
    "\n",
    "    ### ALADIN ### \n",
    "\n",
    "    if dataset == 'ALADIN':\n",
    "        \n",
    "        if var == 'tas' or var == 'pr':\n",
    "            ds = xr.open_mfdataset(data_path+'ALADIN/ECMWF-ERAINT/tas_pr/*.nc', chunks = 'auto')\n",
    "            \n",
    "        elif var == 'snd' or var == 'snc':\n",
    "            ds = xr.open_mfdataset(data_path+'ALADIN/ECMWF-ERAINT/ALADIN_SL2D/*.nc', chunks = 'auto')    \n",
    "\n",
    "        ## Pre-treatment\n",
    "        ds = ds.rename({'time_counter':'time','nav_lat':'lat','nav_lon':'lon'}).isel(x = slice(8,413),y = slice(8,269)) # rename and remove I zone\n",
    "        ds_ZS = xr.open_dataset('/cnrm/cen/users/NO_SAVE/monteirod/ALADIN/fx/orog_MED-11_ECMWF-ERAINT_evaluation_r1i1p1_CNRM-ALADIN63_v1_fx.nc').rename({'orog':'ZS'}).drop(['lon_bnds','lat_bnds'])\n",
    "        ds = u.sellonlatbox(ds,['x','y'],lat_inf,lat_sup,lon_inf,lon_sup)\n",
    "        ds_ZS = u.sellonlatbox(ds_ZS,['x','y'],lat_inf,lat_sup,lon_inf,lon_sup)\n",
    "        ds = xr.merge([ds_ZS,ds]).set_coords('ZS')\n",
    "        ds = ds.resample(time ='1MS').mean(dim = 'time')\n",
    "\n",
    "        ## Post-treatment\n",
    "        if var == \"tas\":\n",
    "            ds['tas'] = ds['tas'] - 273.15\n",
    "\n",
    "        elif var == 'pr':\n",
    "            ds['pr'] = ds['pr']*3600*24\n",
    "        \n",
    "        elif var == 'snc':\n",
    "            ds['snc'] = ds['snc']*100\n",
    "\n",
    "    ### AROME ###\n",
    "\n",
    "    if dataset == 'AROME':\n",
    "        \n",
    "        data_path = '/cnrm/cen/users/NO_SAVE/monteirod/'\n",
    "        ds = xr.open_mfdataset(data_path+'AROME/ECMWF-ERAINT/'+var+'/'+var+'_ALP-3_ECMWF-ERAINT_evaluation_r1i1p1_CNRM-AROME41t1_fpsconv-x1n2-v1_day_*.nc', chunks = 'auto')\n",
    "\n",
    "        ## Pre-treatment\n",
    "        ds_ZS = xr.open_dataset(data_path+'AROME/fx/orog_ALP-3_ECMWF-ERAINT_evaluation_r0i0p0_CNRM-AROME-41_fpsconv-x2yn2v1_fx.nc').rename({'X':'x','Y':'y', 'orog':'ZS', 'latitude':'lat','longitude':'lon'})\n",
    "        \n",
    "        # Reindex the same ways the orography file and main file\n",
    "        ds_ZS['x'] = ds['x']\n",
    "        ds_ZS['y'] = ds['y']\n",
    "\n",
    "        ds = u.sellonlatbox(ds,['x','y'],lat_inf,lat_sup,lon_inf,lon_sup)\n",
    "        ds_ZS = u.sellonlatbox(ds_ZS,['x','y'],lat_inf,lat_sup,lon_inf,lon_sup).drop(['lon','lat'])\n",
    "        ds = xr.merge([ds,ds_ZS]).set_coords(['ZS','lat','lon'])\n",
    "\n",
    "        if scd == True:\n",
    "            var = 'scd'\n",
    "            ds = ds['snd'].resample(time = 'A-SEP', closed = 'left',label = 'left').map(u.lcscd, threshold = 0.01)\n",
    "            \n",
    "        elif sd == True:\n",
    "            var = 'sd'\n",
    "            ds = ds['snd'].resample(time = '1MS', closed = 'left',label = 'left').map(u.lcscd, threshold = 0.01)\n",
    "        \n",
    "        else:\n",
    "            ds = ds.resample(time = '1MS', closed = 'left',label = 'left').mean()\n",
    "\n",
    "        if var == 'tas':\n",
    "            ds['tas'] = ds['tas'] - 273.15\n",
    "\n",
    "        if var == 'pr':\n",
    "            ds['pr'] = ds['accpluie']*3600 + ds['accneige']*3600 + ds['graupel']*3600\n",
    "\n",
    "            \n",
    "    ### ERA5 ###\n",
    "\n",
    "    if dataset == 'ERA5':\n",
    "\n",
    "        ds = xr.open_mfdataset(data_path+\"ERA5/\"+var+'/*.nc', chunks = 'auto')\n",
    "\n",
    "        ## Pre-treatment\n",
    "        ds_ZS = (xr.open_dataset('/cnrm/cen/users/NO_SAVE/monteirod/ERA5/fx/ERA5_geopotential.nc').squeeze()/9.80665)\n",
    "        ds_ZS = ds_ZS.rename({'z':'ZS','latitude':'lat','longitude':'lon'})\n",
    "        \n",
    "        if var == 'snd':\n",
    "            ds = ds.sel(longitude = slice(4,17),latitude = slice(50,43))\n",
    "            ds_ZS = ds_ZS.sel(lon = slice(4,17),lat = slice(50,43))\n",
    "        \n",
    "        ds = ds.rename({'latitude':'lat','longitude':'lon'})\n",
    "\n",
    "        ds = u.sellonlatbox(ds,['lon','lat'],lat_inf,lat_sup,lon_inf,lon_sup)\n",
    "        ds_ZS = u.sellonlatbox(ds_ZS,['lon','lat'],lat_inf,lat_sup,lon_inf,lon_sup).drop(['lon','lat'])\n",
    "        ds = xr.merge([ds_ZS,ds]).set_coords(['ZS','lat','lon'])\n",
    "\n",
    "        if var == 'tas':\n",
    "            ds = ds.rename({'t2m':'tas'})\n",
    "            ds['tas'] = ds['tas']-273.15\n",
    "            \n",
    "        elif var == 'pr':\n",
    "            ds['pr'] = ds['tp']*1000\n",
    "        \n",
    "        elif var == 'snd':\n",
    "            ds['snd'] = (ds['sd']*1000)/ds['rsn']\n",
    "            ds = ds.drop(['sd','rsn'])\n",
    "\n",
    "        if scd == True:\n",
    "            var = 'scd'\n",
    "            ds = ds['snd'].resample(time = 'A-SEP', closed='left', label='left').map(u.lcscd, threshold = threshold_scd)\n",
    "            \n",
    "        elif sd == True:\n",
    "            var = 'sd'\n",
    "            ds = ds['snd'].resample(time = '1MS', closed = 'left',label = 'left').map(u.lcscd, threshold = threshold_scd)\n",
    "        \n",
    "        else:\n",
    "            ds = ds.resample(time = '1MS', closed = 'left',label = 'left').mean()\n",
    "            \n",
    "\n",
    "    ### ERA5-CROCUS ###\n",
    "\n",
    "    if dataset == 'ERA5_CROCUS':\n",
    "\n",
    "        data_path = '/cnrm/cen/users/NO_SAVE/monteirod/'\n",
    "\n",
    "        ds = xr.open_mfdataset(data_path+'ERA5-CROCUS/'+var+'/'+var+'*.nc', chunks = 'auto')\n",
    "        ds_ZS = xr.open_mfdataset(data_path+'ERA5-CROCUS/'+'fx/*.nc', chunks = 'auto')\n",
    "        \n",
    "        ## Pre-treatment\n",
    "        ds['lat'] = ds_ZS['lat']\n",
    "        ds['lon'] = ds_ZS['lon']\n",
    "\n",
    "        ds = u.sellonlatbox(ds,['lon','lat'],lat_inf,lat_sup,lon_inf,lon_sup)\n",
    "        ds_ZS = u.sellonlatbox(ds_ZS,['lon','lat'],lat_inf,lat_sup,lon_inf,lon_sup).drop(['lon','lat'])\n",
    "        ds = xr.merge([ds_ZS,ds]).set_coords(['ZS','lat','lon'])\n",
    "\n",
    "        if scd == True:\n",
    "            var = 'scd'\n",
    "            ds = ds['snd'].resample(time = 'A-SEP', closed = 'left',label = 'left').map(u.lcscd, threshold = threshold_scd)\n",
    "            \n",
    "        elif sd == True:\n",
    "            var = 'sd'\n",
    "            ds = ds['snd'].resample(time = '1MS', closed = 'left',label = 'left').map(u.lcscd, threshold = threshold_scd)\n",
    "        \n",
    "        else:\n",
    "            ds = ds.resample(time = '1MS', closed = 'left',label = 'left').mean()\n",
    "\n",
    "        if var == 'tas':\n",
    "            ds = ds.drop('height')\n",
    "            ds['tas']= ds['tas']-273.15\n",
    "        \n",
    "        elif var == 'pr':\n",
    "            ds['pr']= ds['pr']*3600*24\n",
    "\n",
    "    ### ERA5-Land ###\n",
    "\n",
    "    if dataset == 'ERA5_Land':\n",
    "\n",
    "        ds = xr.open_mfdataset(data_path+\"ERA5-Land/\"+var+'/*.nc', chunks = 'auto')\n",
    "        ds_ZS = xr.open_dataset(data_path+\"ERA5-Land/\"+'fx/oro_ERA5-Land.nc').squeeze().rename({'z':'ZS','latitude':'lat','longitude':'lon'})/9.80665\n",
    "\n",
    "        ds = u.sellonlatbox(ds,['lon','lat'],lat_inf,lat_sup,lon_inf,lon_sup, avoid_reindex = False)\n",
    "        ds_ZS = u.sellonlatbox(ds_ZS,['lon','lat'],lat_inf,lat_sup,lon_inf,lon_sup, avoid_reindex = False).drop(['lon','lat'])\n",
    "        ds = xr.merge([ds_ZS,ds]).set_coords(['ZS','lat','lon'])\n",
    "        \n",
    "        if var == 'tas':\n",
    "            ds['tas'] = ds['t2m']-273.15\n",
    "        elif var == 'pr':\n",
    "            ds['pr'] = ds['tp']*1000\n",
    "        \n",
    "        elif var == 'snd':\n",
    "            ds['snd'] = (ds['sd']*1000)/ds['rsn']\n",
    "            ds = ds.drop(['sd','rsn'])            \n",
    "\n",
    "        if scd == True:\n",
    "            var = 'scd'\n",
    "            ds = ds['snd'].resample(time = 'A-SEP', closed = 'left',label = 'left').map(u.lcscd, threshold = threshold_scd)\n",
    "        \n",
    "        elif sd == True:\n",
    "            var = 'sd'\n",
    "            ds = ds['snd'].resample(time = '1MS', closed = 'left',label = 'left').map(u.lcscd, threshold = threshold_scd)\n",
    "        \n",
    "        else:\n",
    "            ds = ds.resample(time = '1MS', closed = 'left',label = 'left').mean()\n",
    "\n",
    "\n",
    "    ### CERRA-Land ###\n",
    "\n",
    "    if dataset == 'CERRA_Land':\n",
    "\n",
    "        ds = xr.open_mfdataset(data_path+'CERRA-Land/'+var+'/*.nc', chunks = 'auto', combine = 'nested',concat_dim='time', coords = 'minimal', data_vars='minimal')\n",
    "        ds_ZS = xr.open_dataset('/cnrm/cen/users/NO_SAVE/monteirod/UERRA_MESCAN_SURFEX/fx/ZS_lon_lat_UERRA_MESCAN.nc').rename({'LAT':'lat','LON':'lon'})\n",
    "        ds = xr.merge([ds_ZS,ds]).set_coords(['lat','lon','ZS']).drop('Lambert_Conformal')\n",
    "        ds = u.sellonlatbox(ds,['x','y'],lat_inf,lat_sup,lon_inf,lon_sup)\n",
    "\n",
    "        if scd == True:\n",
    "            var = 'scd'\n",
    "            ds = ds['snd'].resample(time = 'A-SEP', closed = 'left',label = 'left').map(u.lcscd, threshold = threshold_scd)\n",
    "        \n",
    "        elif sd == True:\n",
    "            var = 'sd'\n",
    "            ds = ds['snd'].resample(time = '1MS', closed = 'left',label = 'left').map(u.lcscd, threshold = threshold_scd)\n",
    "        \n",
    "        else:\n",
    "            ds = ds.resample(time = '1MS', closed = 'left',label = 'left').mean()\n",
    "\n",
    "        if var == 'tas':\n",
    "            ds['tas'] = ds['tas'] - 273.15\n",
    "            ds = ds.squeeze().drop('height')\n",
    "            \n",
    "    # Prepare the job to be written in memory\n",
    "    write_job = ds.to_netcdf(out_reportory+var+'/'+var+'5cm_'+dataset+'.nc', compute=False)\n",
    "\n",
    "    # Write the job with a progress bar view\n",
    "    with ProgressBar():\n",
    "        print(dataset+var)\n",
    "        write_job.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot rename 'x' because it is not a variable or dimension in this dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m modis \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ls_year)): \u001b[38;5;66;03m# Loop over the MODIS yearly files and merge them into one\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     i_ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReference_Dataset/MODIS_NDSI_500m_Simon/MODIS_NDSI_500m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mls_year\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2000-03\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2009\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Some MODIS file have rounded issues so we homogenized everything\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     i_ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(i_ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m], decimals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/xarray/core/dataset.py:3392\u001b[0m, in \u001b[0;36mDataset.rename\u001b[0;34m(self, name_dict, **names)\u001b[0m\n\u001b[1;32m   3390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m name_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   3391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims:\n\u001b[0;32m-> 3392\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3393\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot rename \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m because it is not a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3394\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable or dimension in this dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3395\u001b[0m         )\n\u001b[1;32m   3397\u001b[0m variables, coord_names, dims, indexes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rename_all(\n\u001b[1;32m   3398\u001b[0m     name_dict\u001b[38;5;241m=\u001b[39mname_dict, dims_dict\u001b[38;5;241m=\u001b[39mname_dict\n\u001b[1;32m   3399\u001b[0m )\n\u001b[1;32m   3400\u001b[0m assert_unique_multiindex_level_names(variables)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot rename 'x' because it is not a variable or dimension in this dataset"
     ]
    }
   ],
   "source": [
    "## MODIS specific case of computation \n",
    "from glob import glob\n",
    "\n",
    "dataset = 'MODIS'\n",
    "ls_year = np.arange(2000, 2010, 1) # Temporal extension of MODIS dataset\n",
    "\n",
    "modis = []\n",
    "\n",
    "for i in range(0, len(ls_year)): # Loop over the MODIS yearly files and merge them into one\n",
    "    i_ds = xr.open_dataset(glob(data_path+'Reference_Dataset/MODIS_NDSI_500m_Simon/MODIS_NDSI_500m_'+\n",
    "                           str(ls_year[i])+'*.nc')[0]).sel(time=slice('2000-03', '2009')).rename({'x':'lon','y':'lat'})\n",
    "\n",
    "    # Some MODIS file have rounded issues so we homogenized everything\n",
    "    i_ds['lat'] = np.round(i_ds['lat'], decimals=3)\n",
    "    i_ds['lon'] = np.round(i_ds['lon'], decimals=3)\n",
    "    \n",
    "    modis.append(i_ds)\n",
    "ds = xr.concat(modis, dim='time')\n",
    "\n",
    "ds = ds.squeeze().rename({'Band1': 'snc'}) # Only keep NDSI fields\n",
    "\n",
    "# Load orography files from a DEM at MODIS resolutions and cut it to match MODIS images spatial extension\n",
    "oro = xr.open_dataset(data_path+'/Reference_Dataset/MODIS_NDSI_500m_Simon/MNT500m.nc').sel(lat=slice(43.5,\n",
    "                                                                                                     48.5), lon=slice(5, 16.5)).rename({'Band1': 'ZS'})\n",
    "oro['lat'] = ds['lat']\n",
    "oro['lon'] = ds['lon']\n",
    "ds = xr.merge([ds, oro]).set_coords('ZS')\n",
    "\n",
    "if scd:\n",
    "    var = 'scd'\n",
    "    ds = ds['snc'].resample(time='A-SEP').map(u.lcscd, threshold=20)\n",
    "\n",
    "elif sd:\n",
    "    var = 'sd'\n",
    "    ds = ds['snc'].resample(time='1MS').map(u.lcscd, threshold=20)\n",
    "\n",
    "# Spatial subsampled\n",
    "ds = u.sellonlatbox(ds, ['lat', 'lon'], lat_inf, lat_sup, lon_inf, lon_sup)\n",
    "\n",
    "ds.to_netcdf(out_reportory+var+'/'+var+'_'+dataset+'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MODIS computation : NDSI from NSIDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "MODIS_NSIDCscd\n",
      "[########################################] | 100% Completed | 16.73 s\n",
      "1\n",
      "MODIS_NSIDCscd\n",
      "[########################################] | 100% Completed | 15.71 s\n"
     ]
    }
   ],
   "source": [
    "## MODIS specific case of computation \n",
    "\n",
    "dataset = 'MODIS_NSIDC'\n",
    "var = 'scd'\n",
    "data_path = '/cnrm/cen/users/NO_SAVE/monteirod/' # Roots for the different repertory used\n",
    "\n",
    "## Path of the outputs\n",
    "out_reportory = '/mnt/lfs/d10/mrns/users/NO_SAVE/monteirod/Reference_Dataset/MOD10A1_NSIDC/netCDF/scd/'\n",
    "\n",
    "## Temporal extension of MODIS dataset\n",
    "ls_year = np.arange(2000, 2015, 1)\n",
    "\n",
    "# Try to open all at the time with dask\n",
    "#i_ds = xr.open_mfdataset(data_path+'Reference_Dataset/MOD10A1_NSIDC/netCDF/MOD10A1F_CGF_NSIDC_*.nc', chunks={'x': 100, 'y': 100}, combine = 'nested',concat_dim='time', coords = 'minimal', data_vars='minimal').drop('band')\n",
    "\n",
    "for i in range(0, len(ls_year)): # Loop over the MODIS yearly files and merge them into one\n",
    "    print(i)\n",
    "    i_ds = xr.concat([xr.open_mfdataset(glob(data_path+'Reference_Dataset/MOD10A1_NSIDC/netCDF/MOD10A1F_CGF_NSIDC_'+\n",
    "                           str(ls_year[i])+'*.nc')[0], chunks = 'auto').rename({'x':'lon','y':'lat'}),xr.open_mfdataset(glob(data_path+'Reference_Dataset/MOD10A1_NSIDC/netCDF/MOD10A1F_CGF_NSIDC_'+\n",
    "                           str(ls_year[i]+1)+'*.nc')[0], chunks = 'auto').rename({'x':'lon','y':'lat'})], dim = 'time').drop('band')\n",
    "\n",
    "    # Some MODIS file have rounded issues so we homogenized everything\n",
    "    \n",
    "    if var == 'scd':\n",
    "        var = 'scd'\n",
    "        i_ds = i_ds['NDSI'].resample(time='A-SEP', closed = 'left', label = 'left').map(u.lcscd, threshold=20)\n",
    "\n",
    "    elif var == 'sd':\n",
    "        var = 'sd'\n",
    "        i_ds = i_ds['NDSI'].resample(time='1MS', closed = 'left', label = 'left').map(u.lcscd, threshold=20)\n",
    "\n",
    "    # Spatial subsampled\n",
    "    # Load orography files from a DEM at MODIS resolutions \n",
    "    oro = xr.open_dataset(data_path+'/Reference_Dataset/MOD10A1_NSIDC/fx/DEM_MODIS_NSIDC.nc').rename({'Band1': 'ZS'})\n",
    "    oro['lat'] = list(reversed(i_ds['lat']))\n",
    "    oro['lon'] = i_ds['lon']\n",
    "    i_ds = xr.merge([i_ds, oro]).set_coords('ZS')\n",
    "    \n",
    "    i_ds = i_ds.isel(time = 1) # to only select the timestep where you have a full year\n",
    "\n",
    "    write_job = i_ds.to_netcdf(out_reportory+var+'_'+dataset+'_'+str(ls_year[i])+'.nc', compute=False)\n",
    "\n",
    "    # Write the job with a progress bar view\n",
    "    with ProgressBar():\n",
    "        print(dataset+var)\n",
    "        write_job.compute()\n",
    "        \n",
    "## Concat all yearly files of scd into one\n",
    "\n",
    "var = 'scd'\n",
    "dataset = 'MODIS'\n",
    "\n",
    "ls_year = np.arange(2000, 2015, 1)\n",
    "ds = []\n",
    "for i in range(0,len(ls_year)):\n",
    "    ds.append(xr.load_dataset(out_reportory+'scd_MODIS_NSIDC_'+str(ls_year[i])+'.nc'))\n",
    "ds = xr.concat(ds, dim = 'time')\n",
    "\n",
    "out_reportory = '/cnrm/cen/users/NO_SAVE/monteirod/dataset_monthly/'\n",
    "\n",
    "ds.to_netcdf(out_reportory_repertory+var+'_'+dataset+'_NSIDC.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Shapefile production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import cascaded_union\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "#######################\n",
    "## Alpine shp area  ###\n",
    "#######################\n",
    "\n",
    "## HISTALP intersect ALPCONV ## # Intersect the HISTALP division with the shapefile of the Alps (as defined by the Alpine convention)\n",
    "HISTALP = geopandas.read_file('/cnrm/cen/users/NO_SAVE/monteirod/Shapefile/HISTALP_region/')\n",
    "alp_conv = geopandas.read_file('/cnrm/cen/users/NO_SAVE/monteirod/Shapefile/Alpine_Convention_Perimeter_2018_v2_4326/')\n",
    "\n",
    "HISTALP_Alps = geopandas.overlay(HISTALP, alp_conv, how='intersection') # Intersect both shapefile\n",
    "NAME = ['SW', 'NW', 'NE', 'SE'] # Labelled the created regions \n",
    "HISTALP_Alps['NAME'] = NAME\n",
    "HISTALP_Alps.to_file('/cnrm/cen/users/NO_SAVE/monteirod/Shapefile/HISTALP_Alps', encoding='UTF-8') # Save it\n",
    "\n",
    "## NUTS0 intersect ALPCONV ## # Intersection of the NUTS0 division with Alpconv\n",
    "NUTS = geopandas.read_file('/cnrm/cen/users/NO_SAVE/monteirod/Shapefile/NUTS/NUTS_RG_20M_2021_4326.shp')\n",
    "NUTS0 = NUTS[NUTS['LEVL_CODE'].isin([0])]\n",
    "alp_conv = geopandas.read_file('/cnrm/cen/users/NO_SAVE/monteirod/Shapefile/Alpine_Convention_Perimeter_2018_v2_4326/')\n",
    "NUTS0_UE_ALPS = NUTS0.overlay(alp_conv, how='intersection')\n",
    "\n",
    "# Select Swiss (CH) and Lichtenstein (LI) row from the intersect shapefile\n",
    "CH = NUTS0_UE_ALPS.iloc[3:4]\n",
    "LI = NUTS0_UE_ALPS.iloc[7:9]\n",
    "\n",
    "# Create one row based on the two polygons\n",
    "CHLI = CH.overlay(LI, how='union').dissolve()\n",
    "\n",
    "# Removed Li and Hu country from our geodataframe #\n",
    "NUTS0_UE_ALPS = NUTS0_UE_ALPS[NUTS0_UE_ALPS['NUTS_ID'] != 'LI']\n",
    "NUTS0_UE_ALPS = NUTS0_UE_ALPS[NUTS0_UE_ALPS['NUTS_ID'] != 'HU']\n",
    "\n",
    "# Replace the geometry of CH by our new geometry that includes LI inside CH\n",
    "NUTS0_UE_ALPS['geometry'][3] = CHLI['geometry'][0]\n",
    "\n",
    "NUTS0_UE_ALPS.to_file('/cnrm/cen/users/NO_SAVE/monteirod/Shapefile/NUTS0_UE_ALPS', encoding='UTF-8')\n",
    "\n",
    "#######################\n",
    "## Mountainous area ###\n",
    "#######################\n",
    "\n",
    "# Load the shapefile that gathered all european mountain regions \n",
    "shp = geopandas.read_file('/cnrm/cen/users/NO_SAVE/monteirod/Shapefile/EuropeanMountainAreas/')\n",
    "\n",
    "## Create shapefile for the specific regions and only keep the largest regions\n",
    "\n",
    "# For Pyrenees\n",
    "Pyrenees = shp[shp['name_mm'] == 'Pyrenees']\n",
    "Pyrenees = Pyrenees.where(Pyrenees['area_km2'] > 800).dropna()\n",
    "\n",
    "# For the Carpathian\n",
    "Carpates = shp[shp['name_mm'] == 'Carpathians']\n",
    "Carpates = Carpates.where(Carpates['area_km2'] > 800).dropna()\n",
    "\n",
    "# For Scandinavia\n",
    "Scandinavia = shp[shp['name_mm'] == 'Nordic mountains']\n",
    "\n",
    "# Create square surrounding Scandinavia because Nordic mountains include icelands that we want to exclude from the region\n",
    "polygon = Polygon([(4*10**6, 3.75*10**6), (4*10**6, 5.50*10**6), (5.50*10**6,\n",
    "                  5.50*10**6), (5.50*10**6, 3*10**6), (4*10**6, 3.75*10**6)])\n",
    "poly_gdf = geopandas.GeoDataFrame([1], geometry=[polygon])\n",
    "\n",
    "# Clip the square over Scandinavie shp, meaning that you only keep polygon inside your square\n",
    "Scandinavia = Scandinavia.clip(poly_gdf)\n",
    "Scandinavia = Scandinavia.where(Scandinavia['area_km2'] > 800).dropna()\n",
    "\n",
    "# # Resume all polygons for a region into one\n",
    "Pyrenees = Pyrenees.to_crs(\"EPSG:4326\")\n",
    "Carpates = Carpates.to_crs(\"EPSG:4326\")\n",
    "Scandinavia = Scandinavia.to_crs(\"EPSG:4326\")\n",
    "Pyrenees.dissolve().to_file('/cnrm/cen/users/NO_SAVE/monteirod/Shapefile/Pyrenees_shp', encoding='UTF-8')\n",
    "Carpates.dissolve().to_file('/cnrm/cen/users/NO_SAVE/monteirod/Shapefile/Carpates_shp', encoding='UTF-8')\n",
    "Scandinavia.dissolve().to_file('/cnrm/cen/users/NO_SAVE/monteirod/Shapefile/Scandinavia_shp', encoding='UTF-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
